{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jak8c76wAoD-"
   },
   "source": [
    "![Texte alternatif…](https://eumas2017.ibisc.univ-evry.fr/img/ueve.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eo5iSblpDAS0"
   },
   "source": [
    "# **Ayoub Abraich**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbPKMXi0xZnO"
   },
   "source": [
    "\n",
    "# Projet de Machine Learning : NLP & Analyse Sentimnetal sur IMDB  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHrhIRxgxZnR"
   },
   "source": [
    "\n",
    "## Plan :\n",
    "- Prétraitement des données\n",
    "- Analyse Sentimnetal en utilisant Bags Of Words & Random forest\n",
    "- Analyse Sentimnetal avec le deep learning\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DRga4kmb_H_C"
   },
   "source": [
    "Afin d'éviter les erreurs d'importation des packages , j'ai utilisé Google Colab ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9wi5kfGdhK0R"
   },
   "source": [
    " ### Import des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "qr_Ey0dRxfDA",
    "outputId": "8d879fd8-8d58-4800-8ed7-8d2fb5a39e64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "import nltk    \n",
    "nltk.download('stopwords')\n",
    "  \n",
    "from nltk.corpus import stopwords                   #Stopwords corpus\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cXs2uuDn8Cnh",
    "outputId": "2a1cac71-7bf5-444e-8381-dfa327b03537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1fr51oVCHRZU"
   },
   "source": [
    "\n",
    "# Prétraitement des données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzToMR455LFB"
   },
   "source": [
    "### Analyse Sentimnetal en utilisant Bags Of Words & Random forest \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "0_MniTifcYsM",
    "outputId": "67608bf2-666d-47b1-af13-3dc8f05bc6e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preprocessing de Data_train terminé ! ---\n",
      "\n",
      "--- Train Features crées ! --- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Lecture des données\n",
    "data_train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/labeledTrainData.tsv',delimiter = \"\\t\")\n",
    "data_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/testData.tsv', delimiter = \"\\t\")\n",
    "\n",
    "\n",
    "############### Preprocessing : Data_train #########################\n",
    "\n",
    "def reviewWords(review):\n",
    "    data_train_Exclude_tags = re.sub(r'<[^<>]+>', \" \", review)      # Exclure les balises HTML\n",
    "    data_train_num = re.sub(r'[0-9]+', 'number', data_train_Exclude_tags)  # Conversion de nombres en \"NUMBER\"\n",
    "    data_train_lower = data_train_num.lower()              # Conversion en minuscule.\n",
    "    data_train_split = data_train_lower.split()            # Diviser en mots individuels\n",
    "    stopWords = set(stopwords.words(\"english\") )\n",
    "\n",
    "    meaningful_words = [w for w in data_train_split if not w in stopWords]     # Supprimer les stop words.\n",
    "    \n",
    "    return( \" \".join( meaningful_words ))   \n",
    "\n",
    "# Nettoyage de données et traitement de texte.\n",
    "cleanWords = []\n",
    "for i in range(data_train['review'].size):\n",
    "    cleanWords.append( reviewWords( data_train[\"review\"][i] ))\n",
    "print(\"--- Preprocessing de Data_train terminé ! ---\\n\")\n",
    "\n",
    "# Creation des  features à partir des bags of words.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000)\n",
    "data_train_features = vectorizer.fit_transform(cleanWords).toarray()\n",
    "print(\"--- Train Features crées ! --- \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "gLXCQ_wb3Is-",
    "outputId": "e247392a-f259-41e0-caff-f299b09d1292"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preprocessing de Data_test terminé ! ---\n",
      "\n",
      "--- Test Features crées !--- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "############### Preprocessing : Data_test #########################\n",
    "# Traitons toutes les revues de test ensemble.\n",
    "testcleanWords = []\n",
    "for i in range(data_test['review'].size):\n",
    "    testcleanWords.append( reviewWords( data_test[\"review\"][i] ))\n",
    "print(\"--- Preprocessing de Data_test terminé ! ---\\n\")\n",
    "\n",
    "# Créer des features à partir de bags of words.\n",
    "data_test_features = vectorizer.transform(testcleanWords).toarray()\n",
    "print(\"--- Test Features crées !--- \\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U7jC7tTXwg-g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8oY8t3Sn-bp"
   },
   "outputs": [],
   "source": [
    "\n",
    "#################### Fonction : pred qui retourne les valeurs prédites + le score de prédiciton ##########\n",
    "\n",
    "def pred(data_train_features,data_train,data_test_features,data_test,clf,hyp):\n",
    "     # Apprentissage\n",
    "    print(\"Entraîner le classificateur :\\n\")\n",
    "    classifier = clf(n_estimators = hyp)\n",
    "    fit = classifier.fit(data_train_features, data_train[\"sentiment\"])\n",
    "    print(\"......... : Done ! \\n\")\n",
    "\n",
    "    score_train = classifier.score(data_train_features, data_train[\"sentiment\"])\n",
    "\n",
    "    print(\"Score_train: %f\" %(score_train))\n",
    "    # Predictions.\n",
    "    result = classifier.predict(data_test_features)\n",
    "    score_test = classifier.score(data_test_features, result)\n",
    "    print(\"Score_test: %f\" %(score_test))\n",
    "\n",
    "    # Mettre les valeurs prédites dans un data frame avec une colonne id et une colonne de sentiments.(0/1)\n",
    "    output = pd.DataFrame(data = {\"id\": data_test[\"id\"], \"sentiment\": result} )\n",
    "    return output\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2071
    },
    "colab_type": "code",
    "id": "w1W9DZjyqmew",
    "outputId": "e43c25f7-7db3-404f-91ec-d6b106ff4362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraîner le classificateur :\n",
      "\n",
      "......... : Done ! \n",
      "\n",
      "Score_train: 1.000000\n",
      "Score_test: 1.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2913_8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4396_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>395_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10616_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9074_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9252_3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9896_9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>574_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11182_8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11656_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2322_4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8703_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7483_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6007_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12424_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4672_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10841_3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8954_7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7392_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10288_8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5343_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4950_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9257_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8689_3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4480_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>6857_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24971</th>\n",
       "      <td>11091_8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24972</th>\n",
       "      <td>4167_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24973</th>\n",
       "      <td>679_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24974</th>\n",
       "      <td>10147_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24975</th>\n",
       "      <td>6875_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24976</th>\n",
       "      <td>923_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24977</th>\n",
       "      <td>6200_8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24978</th>\n",
       "      <td>7208_8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24979</th>\n",
       "      <td>5363_8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>4067_8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24981</th>\n",
       "      <td>1773_7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24982</th>\n",
       "      <td>1498_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24983</th>\n",
       "      <td>10497_10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24984</th>\n",
       "      <td>3444_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24985</th>\n",
       "      <td>588_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24986</th>\n",
       "      <td>9678_9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24987</th>\n",
       "      <td>1983_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24988</th>\n",
       "      <td>5012_3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24989</th>\n",
       "      <td>12240_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>5071_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24991</th>\n",
       "      <td>5078_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24992</th>\n",
       "      <td>10069_3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>7407_8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24994</th>\n",
       "      <td>7207_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>2155_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>59_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>2531_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>7772_8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>11465_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  sentiment\n",
       "0      12311_10          1\n",
       "1        8348_2          0\n",
       "2        5828_4          1\n",
       "3        7186_2          1\n",
       "4       12128_7          1\n",
       "5        2913_8          0\n",
       "6        4396_1          0\n",
       "7         395_2          1\n",
       "8       10616_1          0\n",
       "9        9074_9          0\n",
       "10       9252_3          0\n",
       "11       9896_9          1\n",
       "12        574_4          0\n",
       "13      11182_8          1\n",
       "14      11656_4          0\n",
       "15       2322_4          1\n",
       "16       8703_1          0\n",
       "17       7483_1          0\n",
       "18      6007_10          1\n",
       "19      12424_4          0\n",
       "20       4672_1          0\n",
       "21      10841_3          0\n",
       "22       8954_7          1\n",
       "23       7392_1          0\n",
       "24      10288_8          1\n",
       "25       5343_4          0\n",
       "26       4950_1          0\n",
       "27       9257_4          0\n",
       "28       8689_3          0\n",
       "29       4480_2          0\n",
       "...         ...        ...\n",
       "24970   6857_10          1\n",
       "24971   11091_8          1\n",
       "24972    4167_2          1\n",
       "24973     679_4          0\n",
       "24974   10147_1          0\n",
       "24975    6875_1          0\n",
       "24976    923_10          1\n",
       "24977    6200_8          0\n",
       "24978    7208_8          1\n",
       "24979    5363_8          1\n",
       "24980    4067_8          0\n",
       "24981    1773_7          1\n",
       "24982   1498_10          1\n",
       "24983  10497_10          0\n",
       "24984   3444_10          1\n",
       "24985     588_2          0\n",
       "24986    9678_9          1\n",
       "24987    1983_9          0\n",
       "24988    5012_3          0\n",
       "24989   12240_2          1\n",
       "24990    5071_2          0\n",
       "24991    5078_2          0\n",
       "24992   10069_3          1\n",
       "24993    7407_8          1\n",
       "24994    7207_1          0\n",
       "24995   2155_10          1\n",
       "24996     59_10          1\n",
       "24997    2531_1          0\n",
       "24998    7772_8          1\n",
       "24999  11465_10          1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############# Classifieur = Random Forest ################\n",
    "\n",
    "clf1=RandomForestClassifier\n",
    "hyp1=100\n",
    "\n",
    "pred(data_train_features,data_train,data_test_features,data_test,clf1,hyp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uA8ojP_-7R_e"
   },
   "source": [
    "Maintenant , on va importer les données IMDB déja hachées disponible sur Keras afin de classifier les reviews en utilisant deep learning ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4QRgTevQ-Hw4"
   },
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yv2XIwi5hQ_g"
   },
   "source": [
    "## Import des données haché**e**s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y3dN2qDbxZnf"
   },
   "source": [
    "Jeu de données de 25 000 critiques de films de IMDB, étiqueté par sentiment (positif / négatif). Les revues ont été prétraitées et chaque critique est codée sous forme d'une séquence d'index de mots (entiers). Pour plus de commodité, les mots sont indexés par fréquence globale dans l'ensemble de données, de sorte que, par exemple, l'entier \"3\" code le troisième mot le plus fréquent dans les données. Cela permet des opérations de filtrage rapides telles que: \"ne considérez que les 10 000 mots les plus communs, mais éliminez les 20 mots les plus courants\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IMI69GNisMDU",
    "outputId": "3c95d8a1-4c83-4aa7-aea4-b4ebafef3ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000 #vocab size\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ejrKHn4-sZ98"
   },
   "source": [
    "L'argument num_words = 10000 conserve les 10 000 mots les plus fréquents dans les données d'apprentissage. Les mots rares sont supprimés pour que la taille des données reste gérable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJxntUdrskqG"
   },
   "source": [
    "## Exploration des données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UFPqde19s20O"
   },
   "source": [
    "Prenons un moment pour comprendre le format des données. L'ensemble de données est prétraité: chaque exemple est un tableau d'entiers représentant les mots de la critique du film. Chaque étiquette est une valeur entière égale à 0 ou 1, 0 étant un avis négatif et 1 un avis positif.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FoYlg8NHxZng",
    "outputId": "71885c8d-e665-44eb-dbd4-3cdda7287a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrées d'apprentissage: 25000, labels: 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Entrées d'apprentissage: {}, labels: {}\".format(len(train_data), len(train_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JScqY0lwsnhp"
   },
   "source": [
    "Le texte des critiques a été converti en nombres entiers, chaque nombre entier représentant un mot spécifique dans un dictionnaire. Voici à quoi ressemble le premier commentaire:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "h662ywwUw7jq",
    "outputId": "8eda87f4-4b04-4163-ee9a-d0284a4918e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AN_LRQ9NkOjs"
   },
   "source": [
    "Les critiques de films peuvent être de différentes longueurs. Le code ci-dessous indique le nombre de mots dans les première et deuxième évaluations. Comme les entrées dans un réseau de neurones doivent avoir la même longueur, nous devrons résoudre ce problème plus tard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FlQq0SUepQbd",
    "outputId": "e821ccdc-08ab-4924-d6fe-0a352d3c7512"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 189)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rTX3heEtu0b2"
   },
   "source": [
    "## Reconstruire le text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tPVPKWT1uAPw"
   },
   "source": [
    "Il peut être utile de savoir comment convertir les entiers en texte. Ici, nous allons créer une fonction d'assistance pour interroger un objet dictionnaire contenant le mappage entier à chaîne:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hRQT-m7bwBeH",
    "outputId": "fb1fb524-d595-43de-e368-1a927d9b4bdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Un dictionnaire mappant des mots sur un index entier\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Les premiers index sont réservés\n",
    "word_index = {k:(v+3) for k,v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # inconnu\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tag6cS2buci4"
   },
   "source": [
    "Nous pouvons maintenant utiliser la fonction decode_review pour afficher le texte du premier review:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "aRRcQzVjrARP",
    "outputId": "da8c9c6a-e03e-41a7-f2eb-a571cedd0ca9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y8xAnnCTupr7"
   },
   "source": [
    "\n",
    "\n",
    "## Préparer les données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m4Bvas4jwhRu"
   },
   "source": [
    "Nous voulons utiliser le deep learning pour classifier les reviews , mais comme nous avons déja mentionné , les critiques de films peuvent être de différentes longueurs : nous pouvons remplir les tableaux pour qu'ils aient tous la même longueur, puis créer un tenseur entier de forme max_length * num_reviews avec la fonction pad_sequences pour standardiser les longueurs :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZRrUlfkxyqE"
   },
   "outputs": [],
   "source": [
    "max_length=256\n",
    "train_data = pad_sequences(train_data,value=word_index[\"<PAD>\"],padding='post',maxlen=max_length)\n",
    "\n",
    "test_data = pad_sequences(test_data,value=word_index[\"<PAD>\"],padding='post',maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mvbFycVUyTfU"
   },
   "source": [
    "Regardons maintenant la longueur des exemples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Xzw1B-UfyUWR",
    "outputId": "e422152a-686b-41e8-a853-29410c1b4166"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "8p_cy1Tby4x5",
    "outputId": "1a5ae6db-7966-4579-8902-2956e5252ad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      " 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
      "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
      "   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n",
      "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
      "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
      "  103   32   15   16 5345   19  178   32    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0SoXzHwWzBAc"
   },
   "source": [
    "## Construire le modèle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_5BnQeBzKw_"
   },
   "source": [
    "Les données d'entrée consistent en un tableau d'indices de mots. Les étiquettes à prédire sont 0 ou 1.  On va utiliser pour construire un modèle pour ce problème de classification : Réseau neuronal\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CRsZm34kzW1y"
   },
   "source": [
    "## Réseau neuronal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8zMtUAuwzgjX"
   },
   "source": [
    "Le réseau de neurones est créé en superposant des couches, ce qui nécessite deux décisions architecturales principales: \n",
    "\n",
    "\n",
    "*   Combien de couches à utiliser dans le modèle? \n",
    "*   Combien d'unités cachées utiliser pour chaque couche? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "AnoYC3yby_QE",
    "outputId": "affb3991-a563-4142-c76e-3cb3af37c015"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# le nombre de vocabulaire utilisé pour les critiques de films (10 000 mots)\n",
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7JMoPAVf00FY"
   },
   "source": [
    "Les couches sont empilées de manière séquentielle pour construire le classificateur:\n",
    "\n",
    "1.   La première couche est une couche d’incorporation. Cette couche prend le vocabulaire codé en entier et recherche le vecteur d’incorporation pour chaque index de mot. Ces vecteurs sont appris en tant que trains modèles. Les vecteurs ajoutent une dimension au tableau de sortie. Les dimensions résultantes sont: (lot, séquence, incorporation). \n",
    "\n",
    "2.   Ensuite, une couche GlobalAveragePooling1D renvoie un vecteur de sortie de longueur fixe pour chaque exemple en effectuant une moyenne sur la dimension de la séquence. Cela permet au modèle de gérer une entrée de longueur variable, de la manière la plus simple possible. \n",
    "3. Ce vecteur de sortie de longueur fixe est acheminé via une couche entièrement connectée (dense) avec 16 unités cachées. \n",
    "\n",
    "4. La dernière couche est densément connectée à un seul nœud de sortie. En utilisant la fonction d’activation sigmoïde, cette valeur est un flottant compris entre 0 et 1, représentant une probabilité ou un niveau de confiance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPTA34VT1ZkI"
   },
   "source": [
    "**Unités cachées :**\n",
    "\n",
    "Le modèle ci-dessus a deux couches intermédiaires ou \"cachées\", entre l'entrée et la sortie. Le nombre de sorties (unités, nœuds ou neurones) est la dimension de l'espace de représentation pour la couche. En d’autres termes, le degré de liberté dont dispose le réseau pour l’apprentissage d’une représentation interne. \n",
    "\n",
    "Si un modèle a plusieurs unités cachées (un espace de représentation de dimension supérieure) et / ou plusieurs couches, le réseau peut alors apprendre des représentations plus complexes. Toutefois, cela rend le réseau plus onéreux en termes de calcul et peut conduire à l’apprentissage de modèles non souhaités, modèles qui améliorent les performances des données d’apprentissage, mais pas celles des tests. C'est ce qu'on appelle l'overfitting, et nous l'explorerons plus tard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TnEY_B43116A"
   },
   "source": [
    "**Fonction de perte et optimiseur \n",
    "**\n",
    "\n",
    "Un modèle nécessite une fonction de perte et un optimiseur pour la formation. Comme il s'agit d'un problème de classification binaire et que le modèle génère une probabilité (une couche à unité unique avec une activation sigmoïde), nous utiliserons la fonction de perte binary_crossentropy. \n",
    "\n",
    "Ce n'est pas le seul choix pour une fonction de perte, vous pouvez, par exemple, choisir mean_squared_error. Mais, généralement, binary_crossentropy est préférable pour traiter les probabilités - il mesure la \"distance\" entre les distributions de probabilité, ou dans notre cas, entre la distribution de vérité sur le terrain et les prédictions. Plus tard, lorsque nous explorerons des problèmes de régression (par exemple, pour prévoir le prix d’une maison), nous verrons comment utiliser une autre fonction de perte appelée erreur quadratique moyenne.\n",
    "\n",
    "*Maintenant*, configurez le modèle pour utiliser un optimiseur et une fonction de perte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BmV7VrgkA6JU"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWuAv0q_2XYi"
   },
   "source": [
    "**Créer un jeu de données de validation **\n",
    "\n",
    "Lors de l'apprentissage , nous voulons vérifier l'exactitude du modèle avec des données qu'il n'a jamais vues auparavant. Créez un ensemble de validation en mettant de côté 10 000 exemples à partir des données d’entraînement originales. (Pourquoi ne pas utiliser le jeu de tests maintenant? Notre objectif est de développer et d’ajuster notre modèle en utilisant uniquement les données d’entraînement, puis d’utiliser les données de test une seule fois pour évaluer notre précision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "jnpTlwk7ydzY"
   },
   "outputs": [],
   "source": [
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z_X1W_lN29sk"
   },
   "source": [
    "**Former le modèle**\n",
    "\n",
    "\n",
    "Former le modèle pour 40 époques en mini-lots de 512 échantillons. Cela représente 40 itérations sur tous les échantillons des tenseurs x_train et y_train. Pendant l'entraînement, surveillez la perte et la précision du modèle sur les 10 000 échantillons de l'ensemble de validation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2485
    },
    "colab_type": "code",
    "id": "_i18oCw6HNHi",
    "outputId": "d28a167f-50b4-44cb-c510-f3587514f4e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/70\n",
      "15000/15000 [==============================] - 1s 84us/sample - loss: 0.6908 - acc: 0.5880 - val_loss: 0.6874 - val_acc: 0.6455\n",
      "Epoch 2/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.6811 - acc: 0.7269 - val_loss: 0.6747 - val_acc: 0.7492\n",
      "Epoch 3/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.6616 - acc: 0.7643 - val_loss: 0.6507 - val_acc: 0.7679\n",
      "Epoch 4/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.6296 - acc: 0.7788 - val_loss: 0.6158 - val_acc: 0.7732\n",
      "Epoch 5/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.5870 - acc: 0.8043 - val_loss: 0.5736 - val_acc: 0.8010\n",
      "Epoch 6/70\n",
      "15000/15000 [==============================] - 1s 53us/sample - loss: 0.5379 - acc: 0.8257 - val_loss: 0.5289 - val_acc: 0.8158\n",
      "Epoch 7/70\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.4877 - acc: 0.8427 - val_loss: 0.4837 - val_acc: 0.8325\n",
      "Epoch 8/70\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.4410 - acc: 0.8589 - val_loss: 0.4450 - val_acc: 0.8434\n",
      "Epoch 9/70\n",
      "15000/15000 [==============================] - 1s 55us/sample - loss: 0.4000 - acc: 0.8723 - val_loss: 0.4123 - val_acc: 0.8518\n",
      "Epoch 10/70\n",
      "15000/15000 [==============================] - 1s 55us/sample - loss: 0.3650 - acc: 0.8828 - val_loss: 0.3863 - val_acc: 0.8573\n",
      "Epoch 11/70\n",
      "15000/15000 [==============================] - 1s 56us/sample - loss: 0.3359 - acc: 0.8891 - val_loss: 0.3652 - val_acc: 0.8640\n",
      "Epoch 12/70\n",
      "15000/15000 [==============================] - 1s 56us/sample - loss: 0.3115 - acc: 0.8961 - val_loss: 0.3489 - val_acc: 0.8660\n",
      "Epoch 13/70\n",
      "15000/15000 [==============================] - 1s 55us/sample - loss: 0.2913 - acc: 0.9023 - val_loss: 0.3348 - val_acc: 0.8727\n",
      "Epoch 14/70\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.2730 - acc: 0.9080 - val_loss: 0.3244 - val_acc: 0.8768\n",
      "Epoch 15/70\n",
      "15000/15000 [==============================] - 1s 55us/sample - loss: 0.2574 - acc: 0.9123 - val_loss: 0.3159 - val_acc: 0.8760\n",
      "Epoch 16/70\n",
      "15000/15000 [==============================] - 1s 55us/sample - loss: 0.2433 - acc: 0.9175 - val_loss: 0.3087 - val_acc: 0.8776\n",
      "Epoch 17/70\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.2302 - acc: 0.9217 - val_loss: 0.3029 - val_acc: 0.8798\n",
      "Epoch 18/70\n",
      "15000/15000 [==============================] - 1s 53us/sample - loss: 0.2185 - acc: 0.9252 - val_loss: 0.2983 - val_acc: 0.8827\n",
      "Epoch 19/70\n",
      "15000/15000 [==============================] - 1s 53us/sample - loss: 0.2079 - acc: 0.9279 - val_loss: 0.2940 - val_acc: 0.8831\n",
      "Epoch 20/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.1983 - acc: 0.9331 - val_loss: 0.2912 - val_acc: 0.8836\n",
      "Epoch 21/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.1884 - acc: 0.9383 - val_loss: 0.2889 - val_acc: 0.8838\n",
      "Epoch 22/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.1802 - acc: 0.9417 - val_loss: 0.2873 - val_acc: 0.8855\n",
      "Epoch 23/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.1719 - acc: 0.9453 - val_loss: 0.2871 - val_acc: 0.8842\n",
      "Epoch 24/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.1647 - acc: 0.9483 - val_loss: 0.2859 - val_acc: 0.8846\n",
      "Epoch 25/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.1573 - acc: 0.9514 - val_loss: 0.2853 - val_acc: 0.8855\n",
      "Epoch 26/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.1507 - acc: 0.9538 - val_loss: 0.2864 - val_acc: 0.8833\n",
      "Epoch 27/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.1445 - acc: 0.9569 - val_loss: 0.2865 - val_acc: 0.8847\n",
      "Epoch 28/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.1386 - acc: 0.9585 - val_loss: 0.2877 - val_acc: 0.8849\n",
      "Epoch 29/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.1332 - acc: 0.9615 - val_loss: 0.2898 - val_acc: 0.8846\n",
      "Epoch 30/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.1276 - acc: 0.9629 - val_loss: 0.2898 - val_acc: 0.8853\n",
      "Epoch 31/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.1220 - acc: 0.9655 - val_loss: 0.2914 - val_acc: 0.8856\n",
      "Epoch 32/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.1171 - acc: 0.9684 - val_loss: 0.2934 - val_acc: 0.8860\n",
      "Epoch 33/70\n",
      "15000/15000 [==============================] - 1s 52us/sample - loss: 0.1123 - acc: 0.9696 - val_loss: 0.2962 - val_acc: 0.8838\n",
      "Epoch 34/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.1080 - acc: 0.9703 - val_loss: 0.2989 - val_acc: 0.8846\n",
      "Epoch 35/70\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1038 - acc: 0.9723 - val_loss: 0.3023 - val_acc: 0.8834\n",
      "Epoch 36/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0997 - acc: 0.9740 - val_loss: 0.3043 - val_acc: 0.8842\n",
      "Epoch 37/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0955 - acc: 0.9750 - val_loss: 0.3074 - val_acc: 0.8816\n",
      "Epoch 38/70\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.0918 - acc: 0.9767 - val_loss: 0.3115 - val_acc: 0.8822\n",
      "Epoch 39/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0886 - acc: 0.9779 - val_loss: 0.3159 - val_acc: 0.8803\n",
      "Epoch 40/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0847 - acc: 0.9795 - val_loss: 0.3189 - val_acc: 0.8811\n",
      "Epoch 41/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0814 - acc: 0.9807 - val_loss: 0.3219 - val_acc: 0.8814\n",
      "Epoch 42/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.0779 - acc: 0.9821 - val_loss: 0.3258 - val_acc: 0.8794\n",
      "Epoch 43/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0750 - acc: 0.9829 - val_loss: 0.3298 - val_acc: 0.8809\n",
      "Epoch 44/70\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.0721 - acc: 0.9845 - val_loss: 0.3342 - val_acc: 0.8791\n",
      "Epoch 45/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0693 - acc: 0.9848 - val_loss: 0.3385 - val_acc: 0.8787\n",
      "Epoch 46/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0664 - acc: 0.9864 - val_loss: 0.3458 - val_acc: 0.8758\n",
      "Epoch 47/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0640 - acc: 0.9869 - val_loss: 0.3481 - val_acc: 0.8774\n",
      "Epoch 48/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.0612 - acc: 0.9873 - val_loss: 0.3518 - val_acc: 0.8782\n",
      "Epoch 49/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.0590 - acc: 0.9888 - val_loss: 0.3573 - val_acc: 0.8764\n",
      "Epoch 50/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.0563 - acc: 0.9896 - val_loss: 0.3617 - val_acc: 0.8776\n",
      "Epoch 51/70\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.0541 - acc: 0.9900 - val_loss: 0.3668 - val_acc: 0.8767\n",
      "Epoch 52/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0520 - acc: 0.9906 - val_loss: 0.3755 - val_acc: 0.8724\n",
      "Epoch 53/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0503 - acc: 0.9905 - val_loss: 0.3788 - val_acc: 0.8743\n",
      "Epoch 54/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0479 - acc: 0.9916 - val_loss: 0.3821 - val_acc: 0.8754\n",
      "Epoch 55/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.0459 - acc: 0.9923 - val_loss: 0.3871 - val_acc: 0.8765\n",
      "Epoch 56/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.0439 - acc: 0.9929 - val_loss: 0.3950 - val_acc: 0.8724\n",
      "Epoch 57/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0419 - acc: 0.9931 - val_loss: 0.3977 - val_acc: 0.8747\n",
      "Epoch 58/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.0407 - acc: 0.9941 - val_loss: 0.4059 - val_acc: 0.8722\n",
      "Epoch 59/70\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.0386 - acc: 0.9942 - val_loss: 0.4086 - val_acc: 0.8730\n",
      "Epoch 60/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0370 - acc: 0.9947 - val_loss: 0.4144 - val_acc: 0.8717\n",
      "Epoch 61/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0354 - acc: 0.9949 - val_loss: 0.4202 - val_acc: 0.8705\n",
      "Epoch 62/70\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.0339 - acc: 0.9953 - val_loss: 0.4250 - val_acc: 0.8722\n",
      "Epoch 63/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0325 - acc: 0.9958 - val_loss: 0.4315 - val_acc: 0.8697\n",
      "Epoch 64/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0312 - acc: 0.9961 - val_loss: 0.4366 - val_acc: 0.8701\n",
      "Epoch 65/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0300 - acc: 0.9964 - val_loss: 0.4436 - val_acc: 0.8702\n",
      "Epoch 66/70\n",
      "15000/15000 [==============================] - 1s 48us/sample - loss: 0.0291 - acc: 0.9964 - val_loss: 0.4477 - val_acc: 0.8701\n",
      "Epoch 67/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0274 - acc: 0.9972 - val_loss: 0.4531 - val_acc: 0.8682\n",
      "Epoch 68/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0263 - acc: 0.9973 - val_loss: 0.4592 - val_acc: 0.8683\n",
      "Epoch 69/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0255 - acc: 0.9973 - val_loss: 0.4649 - val_acc: 0.8683\n",
      "Epoch 70/70\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.0247 - acc: 0.9976 - val_loss: 0.4710 - val_acc: 0.8686\n"
     ]
    }
   ],
   "source": [
    "epochs=70\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w6-81eNX3gzJ"
   },
   "source": [
    "**Évaluer le modèle **\n",
    "\n",
    "Voyons comment le modèle fonctionne. Deux valeurs seront retournées. Loss (un nombre qui représente notre erreur, les valeurs basses sont meilleures), et la précision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bMBMH_tP3t5X",
    "outputId": "27c93bab-d24d-42e4-d249-3499e864ae13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 36us/sample - loss: 0.5043 - acc: 0.8550\n",
      "[0.5042553922891617, 0.855]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y75zcIKu4BzJ"
   },
   "source": [
    "**Graphique de précision et de perte dans le temps\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FQ_7ovC84JVI",
    "outputId": "72c68086-a53c-4f26-96e9-33849b1e9c5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "YI8KYXQc4Z-1",
    "outputId": "20e339b6-717c-4944-99ac-c882efa2b66e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMYYk6L+PzRPCgKp9PGQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "v19QkZIb5Inj",
    "outputId": "767832c2-99f4-493e-d184-c23de37dc73b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl4VOXZ+PHvLAkhECBAUMIiiOSG\niIqoVbQKlqKtS33Rtmr7c6la0Ipia6vo64JbbUtdwFpNa6lVW5fXFpdX37qjVtwAcSPeiAgKiCaK\nkEBIMjPn98c5EybJTDJZJpnM3J/rykXmmTPn3DMZnvs8y3mOz3EcjDHGZB9/dwdgjDGme1gCMMaY\nLGUJwBhjspQlAGOMyVKWAIwxJktZAjDGmCwV7O4ATNcRkTuAI72HY4BNQI33+CBVrWrDvj4Apqjq\n5y1scyOwXlXvbGfInU5EngXuU9W7O2FfDjACOAg4XlXPau/xROSnqvpn7/dWP1tjOoMlgCyiqudF\nfxeRdcD/U9X/tHNf45LY5rL27LunUdXFwOL2vl5EdgcuAf7s7a/Vz9aYzmAJwDQQkSXAK8CJwNnA\nR8DfgFFAL+A2Vb3Z2zZ69rsXcCOwBPgvIA84U1VfFJG7gTWqer2XcG709jsC+IeqXuzt63LgImA9\n8FfgElUdFSe+c4CLcb+3nwGnqep6ETkTOBbYBhwOhIAfqOr7IrIncD8wGHiNON95ETkG+K2q7hNT\nthKYC7yV6DOI2fZM3GT67ZaOJyLfA24AcoFq4GxVXQksBYZ7Z/77ArXACFXdICIXAufidtcqcI6q\nVnif7XrgUKAEWA2coKo7msSW732mE73j/lNVf+k9tydwN1AMbAFmqeqKFsrXEXPSEH0MbPDew4PA\nJFWd0sJ7RUQuBWZ5f6f/BX4FbASOU9Vl3jazgW+r6n81/XuZzmNjAKapA4C9VXUpcAXwsXdGOg24\nUURGxHnN/sBrqjoe+KP3uniOACZ7x7hARIaLyN64Z7/74VbeP4z3QhEZAvwBmK6qY4E1wJUxmxwD\n/FFVS4AXcBMKwG+A51R1DLAAOCzO7p/FrYBHe8caDQz3ypP9DKLiHk9EgriJ5KeqKsCjwO+915wF\nfKKq41S1LuY9H4JbOU71jv8JbhKN+gFwMm53XhEwI0485wEFwDhgEnCmiHzTe+5PwP2quhduZX1v\nK+UtGQys9Cr/hO/VO/Y5uH/vCcA3cU84HgJ+FLO/GcADSRzXdIAlANPUk6oa8X6/ELgAQFXXApuB\n0XFeU6Wqj3q/rwBGJtj3P1Q1rKqbgM9xWwJHAEtU9TNV3QksivdCVf0C6KeqG7yil4E9YzZZparL\n48RwBO6ZKar6BvBBnH3XAY8D3/OKZgCPqGqoDZ9BVNzjefsaoqqvJYg/nmOBh733DnAXcFTM80+o\n6lfevt8lzueuqjfhtgwcVd0CvA/sKSJ5uONB93ubPgocnKi8lTgBcvC6wVp5r8d4cVd5n/tU4F/e\n8U4WEb+IDAQOxP2bmBSyLiDT1Fcxvx+Ee8Y7EggDQ4l/0rA15vcwEEiw73jbFTY55sZ4LxSRAHCt\n17UQwD2rXZ1EDAObPLclQWwPA3Nwz9r/C7jOK0/2M4hq6XgXisgZuF1JeUBrC3EV4Q7Ux+5rSMzj\nVj93ERkL3Cwi47xtRuB2CQ303sdWAFV1gGoRKY5X3kqcAGFV3RbzONF7HRz7nmK6rF4VkTpgihfj\nU6q6PYnjmg6wFoBpyX24FWOJ1wVRkYJjbAP6xjwemmC7k3HP0I/wuhWuTnL/W4D+MY+LEmz3FDDR\nqzBLgOe98rZ+BnGPJyKHApcC3/PiPyeJ2D8HBsU8HuSVtcXtwHvAOC/+lV75l7iV8iAvPp+I7JWo\nXER8NE8yhfEO2Mp7rcRNAtFtB4lI9D0+gNut9X28VpRJLUsApiVDgOWq6nhnc31oXFl3hjeAI0Vk\nsIj0As5oIZZ1qlrpVRg/TDKWV/H6xr2Kaa94G6lqLW4S+B3wqKqGY47bls8g0fGGAF8An3gDs2cA\nfbyKtR7o6/Wdx3oCODGmgpzllbXFEOAtVQ2LyHRgLNDXe79PA2d62x2N2/2XqNzBHXjfz3tvJ+Oe\n2Sc6ZqL3+hjwPREp9N7vI94xAP6B+9kdCjzZxvdp2sESgGnJlcBiEXkHt9IrA/4sImM66wBeP/nf\ncGfbPI/b7xuva+R+YJCIrPF+vwIYISI3tXKIS4DjReQjYDbwTAvbPozb/fNQTFlbP4NEx/s3btfH\nR7gV7K243SwPA+/gdoNt9rqagIbP5jfAy94MoQHAf7fyfpu6HrhJRN7D7V65BrhGRA7DPTM/XkTW\nettFB2ETlV8H/MLb13hgVYJjJnyv3rjAfNyWyCrc8Zr7vff7Lm4L5ClVrYmzX9PJfHY/ANPdRMTn\nnWEiIscC16vq/t0clukGIvIk8AdVtRZAF7BBYNOtRKQI+EBEJuFOc/whbjeKyTJeq2QUbgvCdAHr\nAjLdSlUrcLs1nsOd1TMQmNedMZmuJyKLcKcAnxkzDdmkmHUBGWNMlrIWgDHGZKkeMwZQUVGVdFOl\nsDCfLVt2tL5hmrB4U8viTb2eFnM2xVtUVOBL9FxGtgCCwUQXoqYnize1LN7U62kxW7yujEwAxhhj\nWmcJwBhjspQlAGOMyVKWAIwxJkuldBaQiEzAXU/8FlX9Q5Pnvg38GneFwSdV9bo4uzDGGJMiKWsB\niEgf4DbcKzzjWQichHvHpKNEpDRVsRhjTHdbvDjIlCn5DB3alylT8rn88l4tPl68ONjwmmCQhrLO\nlMoWQC3u3X8ubfqEd8/Rr1T1U+/xk7i320u0uqAxxqTc4sVBbr01l9Wr/ZSURLjoIvcOnbFlhx0W\n5pVXAm3a5rDDwtx1V27DccrLA5SXB1p8PGtW70ax7SqrYcaMUKe835QvBSEi84DK2C4gb530X6lq\ndN30s4Exqnp5ov2EQmGnp83dNcakxgMPwK9/DatWQWkpXO7VHLFlU6fCkiXJb1NcDJ9+2j3vpy32\n3RfefrtNL0l4IVi6XAmcMMCotlwFV1RUQEVFVYcCSsZtt92CajlfffUlO3fupLh4GP369efXv57f\n6muffPJx+vTpy5QpR8aNd8GCm/jBD06huHhYqsJvt676fDuLxZt6HYm5rWfdu+3msGnTrt7rd9+F\nU09tvM9333V/2rJNT6j8AVatcqioSOYuna6iooKEz3VXAtgE7B7zeBiN732aEvG+aB1pSl1wwc8B\ntzJfu/YjZs++KOnXHnPM8S0+P2fOxe2Oy5hUaLmihpKS/LhdHy11lzStzBN1fcR2j2za1Or5YkYr\nKem8xVK7JQGo6joR6Scio4ANwHHAj1N5zMWLg42+WKnoT4tasWIZDzxwHzt27GD27J/z1lvLWbLk\nOSKRCJMnH8ZZZ83kL38pY8CAAYwePYYnnlhMXV2Y9es/ZurUaZx11kxmz57JL35xCS+88Bzbt1fz\nySfr2bhxAxdeeDGTJx/GfffdzbPPPk1x8TBCoRCnnPJjJk06sCGGN998nbvuupOcnBwKCgq49trf\nkJOTw623/p5Vq94jEAjwq19dxp577hW3zGSX1s7C21NRJ9Ovne2VeXvMmVPXaftKWQIQkQOAm3Bv\n8FAvIt/HvR/ox6q6GDgP71ZwwIOqujpVsYD7RY5nwYLcTk8AAB99tIb77/8Xubm5vPXWcv74x7vw\n+/388IcncPLJP2q07TvvvMO99/4PkUiEH/zgeM46a2aj57/44nN+//uFvPbaUh599J/svfcE/vWv\n/+H++//J9u3bOeWUEznllMb5s6qqiquvvp7i4mFcd91VvP76q/Tq1YsvvvicP/3pblauXMFzzz3D\nl19+2azMEkDmaVrBt9SlYhV1apxzTh1Ll+5qDR16aLjFx9GKfsGCXFavDlBSEmbOnI71WjSVsgSg\nqsuBqS08/xIwOVXHb2r16vgzXhOVd9Ree40lN9dNOnl5ecyePZNAIMDXX3/Ntm3bGm1bWlpKXl6i\n+2vDvvtOBGDIkCFUV1ezYcOn7LnnGHr1yqNXrzzGj9+72WsGDBjAb397PeFwmE2bNnLAAQexZctX\n7LPPfgBMnDiJiRMn8fe//61ZmelZWqrck5mBYpV7Y8OHR9i82RenEk6mok68TXsr7hkzQt4YS+ev\nXpoug8ApV1ISafSljy1PhZycHAA2b/6MBx/8O4sW/Z38/HxOO+2HzbYNBlv+MwQCu+J2HAfHAb9/\nV+Lyxfn/e+ON1zF//q2MGjWam2/+LQB+fwDHafx+45WZ9NHRyr3p40zSsYraPaNOtqJOpvJORU9C\nqmVNArjoorpmzVro3P60eL7++msKCwvJz89H9QM2b95MfX19h/Y5dOhQ1q79iFAoRFVVFR98UN5s\nm+3bq9ltt92pqqpixYrljBkzlvHjS7nvvrv50Y9OZ/XqD3j88UeZNm16s7KLL2526YbpArsq+10D\nqtlSubenMu9IRZ2qM+qeJmsSgPvFqGn0pers/rR4xo4toXfvfM477yz22WciJ5xwIjfd9Fv23Xe/\ndu9z4MBBTJ/+HX7609PZY4/RlJbu3aiVAHDiiT/gvPPOZsSIkfz4x6ezaNGfuOOOReyxx2h+9rNz\nALj44rmMGbMXL7/8YqMykxpt7YdPx8o9cUUd/4w6me6SjlTmpmN6zD2B23JHsJ42j7o98T755ONM\nn/4dAoEAp59+CjfffBtDhuyWoggby4bPt6PiVfaxZ/PpqLWz8JZOmOw7kVodibelO4JlTQsg03z5\n5ZfMnHkGOTm5HHXUd7qs8jftmzLZ3Wfzrc1AsbPw7GQtgDRg8aZWR+ONrfCbVu7poL2Ve2fKtu9E\nV7MWgDEp0Naz+a6eMpkOlbvJXJYATNZKfHX4Ll1d4bv98P6UXPRjTFOWAExWiT3jb+Xyi06X7Nm8\nTVE0XcUSgMlYrc2rD4dTe/yms2rsbN6km/QazephZs36SbOLsO688w/cf/99cbdfsWIZV1xxCQBz\n5/6i2fP//OeD/OUvZQmPt2bNh3zyyXoArr76Mmprd7Y39IwUe8eliRP7MGtWb8rLA4TDbvdOZ07D\nHD48QjDoUFoapqyshrKyGkpLw43KVqzYzqZN1SxZssMqf5OWrAXQAdOnH83zzz/DuHHjG8qWLHme\n2267s9XX/uY3N7f5eC+++DzjxpUycuQeXHPNjW1+faZpaXZOZ/fdJ3M2b5W86WksAXTAtGlHcd55\nZ/Ozn10IwAcflFNUVERR0ZC4yzHHOvbYaTzxxHMsW/YGf/zjrfTrN4BBgwY3LO98ww3zqKj4gpqa\nGs46aya77z6URx/9Fy+++DyFhYVcddVl3HPPg1RXV3HjjddSX1+P3+9n7twr8fl83HDDPIqLh7Fm\nzYeUlAhz517Z6PhPP/1/PPzwgwQCfkaNGsOll/43oVCI66+/ms8//4zc3F5cccU1FBYObFbW0g0m\nUqW1C6s6UuH36uUQDtOuC6CM6ckyJgHMm9eLxx93347fD5FInw7v8/jjQ8ybV5vw+cLCgRQXD2PV\nqvcoLZ3A888/w/Tp3wHiL8ecn5/fbB9lZX9g/vz5DBo0jF/+8kKKi4dRVbWNb3zjEL773ePYuHED\nV145l0WL7uPggyczdeo0SksnNLz+rrvu5LjjTmDatKN44YVnWbToT5x99ixUy7nmml9TWDiQGTOO\noaqqioKCXRV3TU0NN910GwUFBZx//k/56KM1rFr1HoMGDWLevBt49tmn+M9/XiIYDDYrKy0d0+HP\ntjUtnd139oVVCxfubFbBW4VvskHGJIDuMn36d3juuWcoLZ3AK6+8xB13LALiL8ccLwF89tlnjBs3\njoqKKiZOnERtbS0FBf0oL3+fxx77Fz6fn23btiY8vmo55547G4BJkw7k7rvvAmDYsBEMGjQYgMGD\ni9i+vbpRAujXrx+XXebedWz9+o/ZuvVrVD/gwAMPAuDb3z4agN///jfNylKt6fTMzuzOaToTx87u\nTTbLmAQwb15tw9m6O41ue5ccd8qUI7nnnkVMn340I0aMpF+/fkD85ZjjiV3WOXpV9jPP/Jtt27Zx\n++13sW3bNs4557QWIvA1vK6+PoTP5+6v6eJwsVd819fXc/PNv+Puu//BoEGDueSSi7zX+IlEGl9w\nHa+sszXt3tm2rfMqfJtXb0xiNguog/Lz+zBmzFjuueevDd0/0Hw55kRLQA8eXMTatWtxHIe33loO\nuEtIDx1ajN/v58UXn294rc/nI9xk7uL48aWsWLEMgJUrlzcakE5kx47tBAIBBg0azOefb+aDD8oJ\nhUKMG1fKihVvAvDKKy9zzz2L4pZ1VOLZOj7KywNs3Nj+r2XT2TkrVmynvh6biWNMHBnTAuhO06d/\nh+uvv5qrr76uoSzecswzZ/6s2WtnzvwZc+bMYfDgIQ0Luk2d+i3mzv0Fq1a9x7HHfo8hQ4bw17/+\nmf32259bb53fqCvpnHPO5cYbr+Pxxx8hGMzhssuuJBRquaLr338ABx10MOecczp77TWWH/3oNBYu\nvJlFi+5j2bI3vLuXBbniinkMGFDYrKwtUjl4a905xnSMLQaXBjI13qZ9+R3V3gurMvXzTSc9LeZs\nitcWgzNdprOWWhg2LEL//o6d3RuTQpYATKdpesbfkaUWrrqq1ip8Y1LMEoDpkM4647d1c4zpepYA\nTLu194zfBm+NSQ+WAEzSmq6umex8/aZLLVhlb0x6sARgkhLv5inJirfUgjGm+9mFYCYpt96a/FLK\nvXo5jS7GssrfmPRkLQCTUOwAb1tm9NgZvzE9gyUAA7R+xW4iNl/fmJ7LEoCJ27+fbB+/zdc3puey\nBJCl2j9/3yEY9NnqmsZkAEsAWagjV+yWlkZ4//0AFRU7UhCZMaYrWQLIEp11xa57y8TOW+DNGNN9\nbBpoFoie8UfX3K+tTe4CrnPOqaO0NGxTOo3JUNYCyEDtvcOWXbFrTHaxBJBh7IpdY0yyLAFkgPb2\n79sZvzHZzRJAD9eRGT12xm9MdrNB4B4u2TV6hg2L2ICuMaYRawH0ME0HeFWTy+F2xa4xpqmUJgAR\nuQU4BHCAOar6ZsxzJwBXALXAA6r6h1TGkgnaMsBr/fvGmNakrAtIRKYAY1V1MnA2sDDmOT/wB+AY\n4AjgeBEZnqpYMkVblmReuHAnmzZVs2TJDqv8jTFxpXIMYBrwCICqlgOFItLPe24w8LWqVqhqBHgO\n+HYKY+mxFi8OMmVKPkOH9qW8PP6fy+93rH/fGNNmqewC2h1YHvO4wivb5v1eICJjgXXAkcCSlnZW\nWJhPMJj8nPaiooK2RdvN4sX7wAMwa1brr50wwcfbb0c/mwBdsVRDJny+6aynxQs9L2aLt2sHgRsu\nR1VVR0TOABYBW4GPY5+PZ8uW5BcfKyoqoKKiqp1hdr1E8V57bT5uhd6y88+voaKi6874M+XzTVc9\nLV7oeTFnU7wtJY5UJoBNuGf8UcXAZ9EHqvoicDiAiNyI2xIwMVavTtRD5xAM2gCvMaZjUpkAngau\nAcpEZBKwSVUbUpiI/B9wBrAdOB64KYWx9BhNr+qNd2FXaWmEJUtsOWZjTMekLAGo6lIRWS4iS4EI\ncL6InAlsVdXFwJ9xk4QD3KiqlamKpadI9qped0lmY4zpmJSOAajq3CZFb8c89y/gX6k8fk+TaJqn\nzek3xqSCXQncjXZ190BJSX7Cq3rDYdi0qbqLo2u7L7/08f77ft5/309trY/RoyMNPwXeOJTjwI4d\nsGOHjy+/9PH559EfP/X1cMABYQ48MEyfPs33//XXUFHhp64OQiGoq4Nw2EdurkOvXpCX55CXB7W1\nbixffeVjyxYfoRBUVuaycyfs3OmjttY9fjSO7duhd284+OAwhxwSZuLEMHl5u47rOFBVBYEAceMy\npqeyBNBN2nJVb0lJpCtCIhQCVT9VVbsqxx07oLraR3W1j6oqqKryEYnAtm151NdDfb273Ycf+vns\ns8SXlfTv7zRs25pg0GHffSMcdFCY7dthzRo/a9b4qazsyGUrvVrd4rnn3P8OvXo5TJgQIRSCigof\nlZU+6urcuAcPjrDHHg577BFhyBCHr7/2NWzz5Zc+9tgjwre+FWbatBDjx0fwJXcrBmO6hc9xnO6O\nISkVFVVJB9oTpnhNmZKf9Fr9qbywq6YGXnwxwBNP5PD000G2bGlfjTV0aIS9944wYUKYvfeOkJ/v\n8PHH/oafTZt85OZCfr5Dfr7778CBDrvt5v7svnsEx4E33gjw6qtBVq70Ewq5sfj9DiNHOuy1V4Sh\nQyP06gXBIOTmOgQCUFfnntXv3Ak1NT7y8tx9FxY6DBrkMHJkb+rqdpCX51buvXrtiqNPH4feveGr\nr3y89lqA118P8OqrAd57z09eHhQVOQwe7P7U1sInn/j59FNfQ2xReXnu8WKTYHFxhH33DbN9u4+v\nv/axdauP6moYPdph4sQw++8fZv/9IwweHKGy0k9lpZtM6up6U1Gxs6G1snOnj+LiCJMmhdlnH/f9\nR0Ui8MknPtau9TN0qMPYsZEO3fKzvXrC/7lY2RRvUVFBwv/UlgC6ydChfQmHm/9d/H6HceMiDYu9\ndXaff309vPuun9deC/DaawFeeinYcFa+++4Rpk0LUVS0q5LOz4e+fR369XPo29ehoABGjOhDVVV1\nQyWckwP5+Z0WIuB2E737boABAxxGjWpc6bVVe74PoZDb5RPvDN7tknMr68JCh6Iihz593G0rKny8\n8EKA558PsmRJgK++chNCfr7DgAFuslm/vnkCSVZursM++0QYPjzC2rVuy6imZte+evd2KC11E8+Y\nMRH69In+HR3696dZ91ZrHCf+Z9BUT/g/Fyub4rUEkIYStQBKS8NtnuIZicCrrwaoq4PBgx2GDHHP\ngEMh+PBDP6tW+Skvd89qly8PNOqG2XPPCMccU8+xx4bYf/8I/iR6WXrC5xuru+INh+Hrr3306+cm\nyaiaGnj/fT8rVwZYsSJAdbXb0oi2NkaNyiMU2kGvXu7YRE6O25pavjzA8uXu3zEUcls6Y8dGKClx\nx1k2bXKvCFf1J0wwffo4TJsW4phjQkyfHiI/301Iq1YFKC/389FHu1oilZXuOMrQoQ777htm333d\nxDJ4sMOnn/pZt87PJ5/42LzZz5AhQfr3r21o0U2cGGbUqPbVLY4DW7dCZaWP0aPdVl5ny6bvsCWA\nNBE7x3+33Rw2bWpe27alu2fHDnjooRzKynL56KPm+/L5HByn8d9eJMzBB4eZPNkd8Bw2rO1//3T9\nfBPJtHhratxB7qFD41eOO3dCebmfDRv8jcZyNm/289RTQdatc78r0S602BZEVEGBm5AKCx3Wr/e1\nefzF73c4/vgQF15Yxz77NB/Dqqz08fHHPjZs8PPpp342bnR/37DBx6ef+qmudmMaOTLCrFl1nHpq\nPX37Nt7Hxo0+Vq/2s99+YQYObFN4GfedaOW1lgC6W9NB36jhwyNs3uynpCScdHfP1q1wxx253H13\nDl995Sc31+HEE0OMGhVpdPYWicC4cRHGj494/4bp37/j7yUdP9+WWLy7OI6bHJ58MshTTwUJhWD8\nePc7UloaZuxYd3C76SyozZt9vPOOn7ffDrB1q4+RIyPssUeEkSMdiosj5OQUsGrVdjZvdsd7Hnoo\nh3ffdbPTkUeG+NGP6lm/3s/KlW7LZ8OG+Amlb1+HESMiDB/udls99VSQnTvdVtTpp9cxblyEV18N\n8J//BFm/3t2Hz+cwcWKEqVNDTJkSZsAAp+H/QGWlj9pad3B+zz3dllLfvi1/xo7jzjgbMCC57q+u\nYAmghyeAlrp83n8/kHS8r78e4Lzz8tiwwU9hocOZZ9Zx1ln17LZb1/0d0/HzbYnFm3pNY3YceOGF\nALfdlssrrzQelR48OML++0cYO9Ydy3B/3Iq/6QlKZaWPu+/OYdGinEatkH79HA49NERJSYQ33wzw\n5puBpMdViooi7LWXn+LievbYw01k4bA7hfm99/ysWhWgqspNclOmhJg6Nczhh4cYMMB9fV0dbN/u\nDs6HwzT8RMdL/H53/Mjvd2e/NW25tIclgB6eABIN+gaDDvX1vlbjDYdh4cJcfve7XBwHfv7zOmbP\nruuWeenp+Pm2xOJNvZZiXrbMz0svBRk7NsL++7vdjm09s965Ex55JMjWrT4mT3ZnmsV2f1VXwyuv\nuC2D6FhYdEwlJ8dh3bpdM9LWrnW7x+Jdae/3u7PNiosd3nrLbe1EywsK3Iq/LQP4Pp+7v/33d9/7\nhAkRevd2YhJH8305Dt41K1Bb66O+Hk46qTc+X+cnALsOoIuUlETitgDcOf4tj3J99pmP2bPzePnl\nIEOHRrjzzp1MntyGu78b040OPDDCgQd2bPmSvDw45ZTE3aN9+8LRR4c5+uhE/y8alxcWFvD229Ws\nX+/nk0/8OA7svXcYkUjDjLZQCFau9LNkSZCXXgqwbZuvYXZcdPpw9Gw/EHDw+93KOxx2WwaRCHz+\nuY+VKwN8+GGAhx7KiRNXcj78EC67rN0vT8gSQBe56KK6uGMA7ro+jctXr/bzxhsBli1zZ36sXu3H\ncXwcfXSIBQtq2jzgZYxpLBiEkSMdRo4M0zQ5xG4TTV6//GX7jxWJuBczvvWWOxsvHI4mDifuVGOf\nj2bXrJxySmru8WEJIEWa3rz9oovqKCurYcGC3IRz/Ldvh8svz+P++3edKeTnOxx6aJgZM0Kcdlp9\n2gxKGWOS4/e7LX23td++a3qKiqCionPjAksAKRFvmYdZs3pTVlaTcI7/e+/5mTkzjzVrAuyzT5jT\nT6/ngAPCjBvXPVd2GmMyXyrvCZy1Eq3quWBB83LHgdtvh+9+N581awLMmlXHk0/u4Iwz6pkwwSp/\nY0zqWPWSAonu5NW0fN06H5dfnsezz8LAgQ6LFtUwfboN7hpjuoa1AFIg0eqd0fKaGpg/P5fDD+/D\ns88GmTYNXnhhh1X+xpguZQkgBS66KP6Utzlz6njmmQBHHNGH+fN7MWCAQ1lZDc88A0OH9ozrMYwx\nmcO6gFLAndnTeMbPhRfWsW6dnxtv7EUg4HDeeXX88pe1FBSkz+XmxpjsYgkgRWbMCDVM8YxE4Oqr\ne1FWlsvw4RHuvbeGvffumpuMD7UVAAAUBElEQVS8GGNMItYF1EkWLw4yZUo+Q4f2ZcqUfBYvdnNr\nfT3Mnp1HWVkuImH+9393WOVvjEkL1gLoBInm/dfV1fDYYzk880yQAw4I8/e/77CreI0xacMSQCdI\nNO//ssvyqK72ceSRIRYtqrEbihtj0oolgE6QaN5/dbWPqVND3HtvDbnxc4QxxnQbGwPoBInm/efm\nOtx1l1X+xpj0ZAmgEySa93/VVbX069fFwRhjTJIsAXSCGTNClJXVMGZMGHAAh4svrmXmzPruDs0Y\nYxJqNQGIyLiuCKSnO/bYkHeHIh8LFuzk0ks7dgMMY4xJtWQGgf8pIluAvwAPqmr89YyzSLy1/isq\nfKxeHeCMM+o49dT2rfltjDFdqdUEoKp7i8gE4IfAEhFZCfxZVd9MeXRpKNGc/z59HAoKHObOtTN/\nY0zPkNQYgKq+p6pXAb8AxgOPichLIjI2pdGloURz/rdv9zFnTh2DBtmibsaYnqHVFoCI7AGcCZwK\nrAJuAJ4CDgLuAw5OYXxpJ9Gcf3D46U/t7N8Y03MkMwawBLf//1uquimm/A0ReSMlUaWxkpII5eWB\nZuXFxQ69U3PfZmOMSYlkuoD2A1ZHK38ROVdE+gKo6gWpDC4dJZrzf+WVtV0ciTHGdEwyCeCvwO4x\nj/OBe1MTTvqLzvkvLQ3j87n9/RdcUMtJJ9nMH2NMz5JMAhioqgujD1T1ZmBA6kJKfzNmhJg3rxbH\n8XH44SGuuML6/o0xPU8yCaCXiIyPPhCRA4CsXt1m506YOzcPv99h3rxau6OXMaZHSmYQ+OfAoyLS\nHwgAFcBpKY0qzd12Wy5r1/qZObOOffaxm7sYY3qmVlsAqvq6qpYApUCJqo4ni1sAa9f6WLAgl913\nj3DppTbwa4zpuZK5DqAf8P+Awd7jXsBPgOLUhpY+oks/qPrp3Rvq6nzccMNOCgq6OzJjjGm/ZLqA\nHgTWA0cDDwNHAecls3MRuQU4BHeJzDmxy0eIyPm4iSUMLFPVi9oWetdouvTD9u3uv/W20KcxpodL\nZhA4T1XPBdar6q+AI3HXBWqRiEwBxqrqZOBsYGHMc/2AXwGHq+o3gVIROaQ9byDVEi39sHBh1vaC\nGWMyRLKzgPoAfhEZpKpfAWOSeN004BEAVS0HCr2KH6DO++krIkHcawu+anP0XSDR0g+Jl4Qwxpie\nIZla7B7gp8BdQLmIvA9sTuJ1u+POGIqq8MpQ1Z3ANcBa3O6l11V1dRvi7jKJbveYqNwYY3qKZMYA\nylTVARCR54AhwMp2HKthtrzXErgcKAG2Ac+LyH6q+naiFxcW5hMMNl+DJ5Gios4Zob3qKjj11Obl\nV14Z6LRjQOfF21Us3tTqafFCz4vZ4k0uATyP2++Pqm4ENia57000XkKiGPjM+308sFZVKwFE5GXg\nACBhAtiyJfn70BQVFVBRUZX09i2ZNg2+8Y3evPFGkEDAQSTCnDl1TJsWoqKi9dcnozPj7QoWb2r1\ntHih58WcTfG2lDiSSQArReRaYCluvz0Aqvp8K697Grebp0xEJgGbVDX6DtYB40Wkt6rWAAcCTyYR\nS5fbuRNWrQowYkSEZcu221W/xpiMkUwCmOj9e3hMmYPbMkhIVZeKyHIRWQpEgPNF5Exgq6ouFpH5\nwAsiEgKWqurLbQ8/9V58MUB1tY/TTqu3yt8Yk1GSuSXkke3duarObVL0dsxzZUBZe/fdVR5/PAeA\n733PJv4bYzJLMlcCv4x7xt+Iqh6RkojSSF0d/PvfQYYNizBpks36McZklmS6gK6I+T0X+BZQnZpw\n0svLLwfYts3Hqada948xJvMk0wX0YpOiZ0QkLQdsO0t07Z8PPnAvk+jXz270bozJPMl0Ae3ZpGgE\nIKkJp/s1XfsHYP78Xuy1V4QZM+yuX8aYzJFMF9BzMb87uBduzUtJNGkg0do/CxbkWgIwxmSUZLqA\nRouIX1UjACKSo6oZOyXG1v4xxmSLVms1ETkJeDSm6GUR+X7qQupetvaPMSZbJHNaezHuuv1RR3ll\nGem44+J388yZYzd+N8ZklmQSgE9Vt0YfqOo23Ct7M8rGjT7OPTeP+fN7AdC/f4RAwKG0NExZWY31\n/xtjMk4yg8DLRORBYAluwvgOsDyVQXWlxYuDXHttLhs3+gEfo0ZFuP32Gg46KONynDHGNJJMC+BC\n4HHcm8ILcB+QlrdvbKvolM+NGwNEV6tet87Phg024GuMyXzJ1HT5QJ2qXqCqFwKFXlmP19KUT2OM\nyXTJ3hEsdl3/fODe1ITTtVRtyqcxJnslU9MNVNWGG7qr6s3AgNSF1HWGDIm/xINN+TTGZINkbwo/\nPvpARA7EXRSuR6uvd3/isSmfxphskMwsoJ8Dj4pIf9yEUQmcltKousDDDwf58ks/U6eG+OILH6tX\n+ykpcW/3aFM+jTHZIJmlIF4HSkRkBO69gc8AHsO9x2+PFArBrbf2IifH4ZZbdjJsmK32aYzJPsms\nBnoI8BPgZNwWwEzgnymOK6UeeSTIxx/7Oe20Oqv8jTFZK2ECEJFLgDOBPrgzgQ4E/kdVH+ia0FLD\ncdzpn8GgY339xpis1lIL4AbgfeB8VX0BQER6/OlyZaWP1asDTJ8eYuTIHv92jDGm3VpKACNw+/vv\nFJEAcDcZMPunosK94nfYMJvqaYzJbgmngarqZlX9raoKcBawF7CHiDwuIsd0WYSdrLLSTQBFRXb2\nb4zJbkld8qqqL6nqmbgzf/4XuCqVQaVStAUweLAlAGNMdkvmOoAGqloFlHk/PZK1AIwxxpV1i95Y\nC8AYY1xZlwCiLYAhQ2wQ2BiT3bIuAVRUuG/ZWgDGmGyXdQlg9Wo/Pp9DSUlfpkzJZ/HiNg2DGGNM\nxsiqBLB4cZD16/04jo9IxEd5eYBZs3pbEjDGZKWsSgC33GJ3ADPGmKisSgCJ7vRldwAzxmSjrKr5\nRo+OP/PH7gBmjMlGWZUATjwx/o1ebFVQY0w2yqoEMH68e6a/224RgkGH0tIwZWU1dgcwY0xWyqrp\nL9GLwObNq+Wkk6zSN8Zkt6xqAUSXgbB1gIwxJksTgF0FbIwxWZYAbCVQY4zZJasSQEWFD7/fYeBA\nSwDGGJNVCaCy0s/AgQ6BQHdHYowx3S+ls4BE5BbgEMAB5qjqm175MODvMZvuCcxV1X+kMp6KCp/d\nC9gYYzwpSwAiMgUYq6qTRWQ8sAiYDKCqG4Gp3nZBYAnwWKpiAaithW3bfOy3n3X/GGMMpLYLaBrw\nCICqlgOFItIvznZnAv9U1eoUxmIDwMYY00Qqu4B2B5bHPK7wyrY12e4c4KjWdlZYmE8wmHznfVFR\nQaPH69e7/44cmUNRUU7S++kqTeNNdxZvavW0eKHnxWzxdu2VwL6mBSIyGfhAVZsmhWa2bNmR9IGK\nigqoqKhqVPbhhwEgn759a6moSK+1f+LFm84s3tTqafFCz4s5m+JtKXGksgtoE+4Zf1Qx8FmTbY4D\nnk1hDA12XQRmg8DGGAOpTQBPA98HEJFJwCZVbZrCDgLeTmEMDaL3ArYxAGOMcaUsAajqUmC5iCwF\nFgLni8iZIjIjZrOhwBepiiGWLQNhjDGNpXQMQFXnNil6u8nz+6Ty+LFsFpAxxjSWNVcCWwvAGGMa\ny5oEUFnpo6DAIS+vuyMxxpj0kDUJoKLCZ90/xhgTIysSQDgMX37psymgxhgTIysSwJYtPiIRawEY\nY0ysrEgANgBsjDHNZUUCsCmgxhjTXFYkALsZvDHGNJcVCSDaArAuIGOM2SUrEoC1AIwxprmsSAC7\nxgBsGqgxxkRlRQKwlUCNMaa5LEkAPnJzHQp61g2AjDEmpbIiAVRWuheB+Zrdk8wYY7JXxicAx7F1\ngIwxJp6MTwDbt8POnT6bAmqMMU1kfAL44gubAmqMMfFkfALYdRGYTQE1xphYGZ8AbAqoMcbEl/EJ\nwJaBMMaY+DI+AQwfHqFfP4f99rMuIGOMiRXs7gBSbdq0MB9+WG3XABhjTBMZ3wIArPI3xpg4siIB\nGGOMac4SgDHGZClLAMYYk6UsARhjTJayBGCMMVnKEoAxxmQpSwDGGJOlLAEYY0yWsgRgjDFZyhKA\nMcZkKUsAxhiTpSwBGGNMlrIEYIwxWcoSgDHGZClLAMYYk6UsARhjTJZK6R3BROQW4BDAAeao6psx\nz40A7gdygRWqem4qYzHGGNNYyloAIjIFGKuqk4GzgYVNNrkJuElVvwGERWRkqmIxxhjTXCq7gKYB\njwCoajlQKCL9AETEDxwOPOY9f76qfpLCWIwxxjSRyi6g3YHlMY8rvLJtQBFQBdwiIpOAl1X1spZ2\nVliYTzAYSPrgRUUFbQ64O1m8qWXxpl5Pi9niTfEYQBO+Jr8PAxYA64AnRORYVX0i0Yu3bNmR9IGK\nigqoqKhqZ5hdz+JNLYs39XpazNkUb0uJI5VdQJtwz/ijioHPvN8rgfWq+pGqhoHngL1TGIsxxpgm\nUpkAnga+D+B182xS1SoAVQ0Ba0VkrLftAYB2dgCLFweZMiWfoUP7MmVKPosXd2WDxxhj0lvKakRV\nXSoiy0VkKRABzheRM4GtqroYuAi42xsQfhd4vDOPv3hxkFmzejc8Li8PeI9rmDEj1JmHMsaYHiml\np8SqOrdJ0dsxz60BvpmqY996a27c8gULci0BGGMMGXwl8OrV8d9aonJjjMk2GVsblpRE2lRujDHZ\nJmMTwEUX1cUtnzMnfrkxxmSbjE0AM2aEKCurobQ0TDDoUFoapqzMBoCNMSYqo+dFzpgRsgrfGGMS\nyNgWgDHGmJZZAjDGmCxlCcAYY7KUJQBjjMlSlgCMMSZL+RzH6e4YjDHGdANrARhjTJayBGCMMVnK\nEoAxxmQpSwDGGJOlLAEYY0yWsgRgjDFZyhKAMcZkqYxbDVREbgEOARxgjqq+2c0hNSMiE4BHgVtU\n9Q8iMgK4FwgAnwGnqWptd8YYS0R+BxyO+325EXiTNI1XRPKBu4HdgDzgOtxbkaZlvFEi0ht4Dzfe\n50jTeEVkKvA/wPte0bvA70jTeKNE5MfAJUAIuAp4hzSNWUTOBk6LKToQOAy4A7dee0dVz+uMY2VU\nC0BEpgBjVXUycDawsJtDakZE+gC34f4nj7oWuF1VDwfWAGd1R2zxiMiRwATvM/0OcCtpHC9wPLBM\nVacAPwRuJr3jjboC+Mr7Pd3jfVFVp3o/F5Dm8YrIIOBq3HuQHwecQBrHrKp/iX6+uHH/Dff/3RxV\nPQzoLyLf7YxjZVQCAKYBjwCoajlQKCL9ujekZmqBY4BNMWVTgce83x8Hvt3FMbXkJeAH3u9fA31I\n43hV9UFV/Z33cASwgTSOF0BExgGlwBNe0VTSON44ppLe8X4beFZVq1T1M1WdSfrHHHUV8FtgdExv\nRqfFm2ldQLsDy2MeV3hl27onnOZUNQSERCS2uE9M8/MLYGiXB5aAqoaB7d7Ds4EngaPTNd4oEVkK\nDMc943s2zeO9CZgNnOE9Ttvvg6dURB4DBgLXkP7xjgLyvZgLgXmkf8yIyEHAp7jdVltinuq0eDOt\nBdCUr7sDaIe0jFlETsBNALObPJWW8arqocD3gPtoHGNaxSsipwOvqurHCTZJq3iBD3Er/RNwE9Zf\naHwimW7xghvTIOBE4Ezgr6TxdyLGObjjWU11WryZlgA24Z7xRxXjDvCku2pvEBBgGI27h7qdiBwN\n/DfwXVXdShrHKyIHeIPqqOpK3MqpKl3jBY4FThCR13D/w19JGn++qrrR62ZzVPUjYDNuV2taxuv5\nHFiqqiEv5irS+zsRNRVYituTMSimvNPizbQE8DTwfQARmQRsUtWq7g0pKc8CJ3m/nwT8uxtjaURE\n+gPzgeNUNTpImbbxAkcAFwOIyG5AX9I4XlU9WVUPUtVDgLtwZwGlbbwi8mMR+aX3++64s63+SprG\n63ka+JaI+L0B4bT+TgCISDFQrap1qloPfCAi3/SePpFOijfjloMWkd/gVgIR4HxVfbubQ2pERA7A\n7fMdBdQDG4Ef4zb18oD1wE+8P3q3E5GZuH2mq2OKz8CtrNIx3t643RIjgN643RXLgHtIw3hjicg8\nYB3wFGkar4gUAP8ABgC5uJ/vW6RpvFEiMgu3CxPgetypzGkbs1dPXK+q3/UelwJluCftr6vqLzrj\nOBmXAIwxxiQn07qAjDHGJMkSgDHGZClLAMYYk6UsARhjTJayBGCMMVkq05aCMKZNRGQUoMCrTZ56\nQlXnd8L+p+JO5/tma9sa09UsARgDFd7Ki8ZkFUsAxiQgIiHcK3OPxL169ExVfU9EDsa9mK8ed332\n2aq6SkTGAn/G7VrdCfzE21VARO4A9sddDfZYr/wfuIuT5QCPq+oNXfPOjHHZGIAxiQWA97zWwR24\na8iDewXpz1X1SNz7Ddzuld8JzFfVI4BF7FpGezwwz1vuoR44GpgO5Hjr0R+Ku/6P/X80XcpaAMZA\nkYgsaVJ2iffvU96/rwC/EpEBwG4xa7MvAR7wfj/Ye4yqPgANYwAfqOrn3jYbcJdReBy4VkQewl1i\n+y5VjXTeWzKmdZYAjEkwBuDdsyF6Vu7D7e5punaKL6bMIX6rOtT0Nar6hYjsB0zGXVp5mYhMUtWa\ndr0DY9rBmpzGtOxb3r/fxL0X61bgM28cANw7M73m/b4U97aZiMjJIvLrRDsVkaOAY1X1FVW9BKgG\nhqTiDRiTiLUAjInfBRS9Qcv+InIe7mDt6V7Z6cDNIhIGwkD0Bt2zgT+JyPm4ff1nAWMSHFOBv4nI\nJd4+nlbV9Z3xZoxJlq0GakwCIuLgDtQ27cIxJiNYF5AxxmQpawEYY0yWshaAMcZkKUsAxhiTpSwB\nGGNMlrIEYIwxWcoSgDHGZKn/D/xd7/pEa5c8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   \n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Wfatpzj5rKt"
   },
   "source": [
    "*Notez* que la perte d’entraînement diminue à chaque époque et que la précision de l’entraînement augmente avec chaque époque. Ceci est attendu lorsque vous utilisez une optimisation de descente de gradient — cela devrait minimiser la quantité souhaitée à chaque itération. Ce n'est pas le cas pour la perte de validation et la précision - elles semblent culminer après une vingtaine d'époques. Ceci est un exemple de surajustement: le modèle fonctionne mieux sur les données d'apprentissage que sur des données qu'il n'a jamais vues auparavant. Après ce point, le modèle sur-optimise et apprend des représentations spécifiques aux données d'apprentissage qui ne se généralisent pas pour tester des données. Dans ce cas particulier, nous pourrions éviter les sur-ajustements en arrêtant simplement l'entraînement après une vingtaine d'époques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uphVVduE_sHs"
   },
   "source": [
    "## RNN/LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_gWhv0cDHII"
   },
   "source": [
    "**Réseaux de neurones récurrents (RNN)**\n",
    "\n",
    "Un réseau de neurones récurrent (voir Figure 1 ) est un réseau de neurones qui tente de modéliser un comportement dépendant du temps ou d'une séquence, tel que la langue, les cours des actions, la demande d'électricité, etc. Cette opération est effectuée en renvoyant la sortie d'une couche de réseau neuronal à l'instant t à l'entrée de la même couche de réseau à l'instant t + 1.\n",
    "\n",
    "Long Short Term Memory networks - simplement appelés «LSTM» (Voir Figure 2) - sont un type particulier de RNN, capable d’apprendre des dépendances à long terme. Ils ont été introduits par Hochreiter et Schmidhuber (1997) et ont été affinés et popularisés par de nombreuses personnes dans les travaux suivants.1 Ils fonctionnent extrêmement bien sur une grande variété de problèmes et sont maintenant largement utilisés.\n",
    "\n",
    "Les LSTM sont explicitement conçus pour éviter le problème de dépendance à long terme. Se souvenir des informations pendant de longues périodes est pratiquement leur comportement par défaut, ce n'est pas quelque chose qu'ils ont du mal à apprendre!\n",
    "\n",
    "Tous les réseaux de neurones récurrents se présentent sous la forme d'une chaîne de modules répétitifs de réseau de neurones. Dans les RNN standard, ce module répétitif aura une structure très simple, telle qu’une seule couche *tanh* .\n",
    "\n",
    "\n",
    "\n",
    "1.   Figure 1: RNN\n",
    "![Figure 1: RNN](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)\n",
    "2.   Figure 2 : LSTM\n",
    "\n",
    "\n",
    "\n",
    "![ Figure 2 : LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "UzCPQfNP_q35",
    "outputId": "a3ecab69-d12d-42dd-c108-b83e8bb33f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 414s 17ms/step - loss: 0.4664 - acc: 0.7756\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 412s 16ms/step - loss: 0.3058 - acc: 0.8766\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 416s 17ms/step - loss: 0.2614 - acc: 0.8978\n",
      "Accuracy: 87.58%\n"
     ]
    }
   ],
   "source": [
    "# fixer un random seed pour la reproductibilité\n",
    "np.random.seed(7)\n",
    "# charger le jeu de données mais ne garde que les n premiers mots, remet à zéro le reste\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# pader les matrices pour éviter le problème de non compatibilité des dimensions\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# Créer le modele\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Evaluation du modèle \n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient au final une accuracy de 87.71%% avec  LSTM mais on perds beacoup on niveau de la compléxité temporelle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Projet ML : NLP",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
